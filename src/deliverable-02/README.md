# Deliverable 2 Summary

Below are explanations regarding the changes made from deliverable 1 to deliverable 2. The main credibility scoring class can be found under [url_validatory.py](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/url_validator.py), with sample unit tests that are scored manually and programatically under [testing/sample.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/testing/sample.csv). The citation scoring portion of the URLValidator requires access to a SerpAPI key which can be hardcoded or maintained using a local file called _api_keys.py_. A utility script, [generate_scoring_lookup.py](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/generate_scoring_lookup.py) was used to create lookup tables in [tables/](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/tree/main/src/deliverable-02/tables) to speed up calculations. A list of trusted domains was created using ChatGPT (and partially verified manually) under [tables/trusted_domains.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/tables/trusted_domains.csv) in order to simulate an expert-curated list of trusted domains.

A Jupyter notebook is available for testing individual user prompts and URLs at [test.ipynb](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/test.ipynb). A Python script for batch testing can be found at [automatic_unit_testing.py](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/automatic_unit_testing.py).

## URLValidator Class

The URLValidator class has four main scoring criteria: Domain Trust, Content Relevance, Title Relevance, and Citation Scoring. The Bias Detection and Fact Check modules from deliverable 1 were removed, as they were nonfunctional and were nontrivial to bring up to production-level quality. All private methods and internal variables in the URLValidator class use the convention of starting with an underscore (_), with the exception of the _rate_url_validity()_ function which is the sole API intended to be accessed publicly by the user.

### Scraping Webpage Content

The first step in the URLValidator class is to scrape the webpage content. Unlike in deliverable 1, the webscraping process is split into three parts. In the first part, the webpage is scraped using requests and converted to a soup object using BeautifulSoup. This allows the class to check if the webscraping was successful. If the webscraping failed, instead of terminating evaluation, the remaining modules that can be evaluated without using scraped data are still executed.

The remaining two parts of the webpage scraping process are to extract textual content from the webpage and to extract all outgoing links from the webpage. The textual content that is extracted are any text blocks within _span_ or _p_ tags that are not assigned an html class, as well as any text with the _#text_ CSS selector. The text is processed by removing all non-alphanumeric characters and converting to lowercase, with duplicate or empty text boxes removed. In an attempt to remove meaningless junk text that is collected, only text boxes containing a number of tokens greater than or equal to 3 times the length of the query are preserved.

The outgoing links are extracted from the webpage by searching for all _a_ tags containing _href_ attributes. These links are then simplified to their base form of subdomain-domain-suffix (ex: www.google.com instead of www.google.com/extra-text). Only unique base urls are preserved, and all urls on the webpage which share the same domain (ex: finance.yahoo.com and sports.yahoo.com share the same domain of yahoo) are discarded, as a website should not gain credibility by linking to itself.

### Domain Trust Rating

The domain trust rating follows the process outlined in [docs/Credibility_Algorithm_v3.pdf](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/docs/Credibility_Algorithm_v3.pdf), with the only exception being that the formulas used are exact rather than algorithmically generated. First, the domain credibility of the webpage is assessed and converted to a contribution score. Then, the domain of each outgoing link is assessed, converted into a contribution score using a decaying exponent for successive links, and added to the webpages innate credibility. Finally, the contribution score is converted back to a star rating using a hyperbolic tangent function. The reason this function is not linear is that star ratings are not typically assigned linear importance from a psychological standpoint. A domain with a trust of 4.5 stars is much more trustworthy than one that is 4.0 stars, but the difference from 3.0 to 3.5 stars is much smaller than the difference from 4.0 to 4.5. As credibility approaches the maximum, it becomes successively harder to attain more credibility, implying asymptotic behavior that is handled by the hyperbolic tangent transformation.

A useful consequence of this process is that the domain trust can be partially evaluated even if the webpage was unable to be scraped. The outgoing links cannot be evaluated without scraping, but a score can be assigned based on the innate domain trustworthiness. The innate domain rating can be assessed via the lookup table provided by the [trusted_domains.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/tables/trusted_domains.csv) file. 

### Content Relevance Scoring

The content relevance module utilizes the 'all-mpnet-base-v2' SentenceTransformer from HuggingFace to compare semantic similarity between the page text and the user prompt, as in deliverable 1. However, the process is modified to greatly improve accuracy. Rather than comparing the user query to the entire text content of the webpage, the user query is compared to each individual text block extracted, as outlined in the scraping webpage content section above. The user query is also preprocessed to remove special characters and converted to lowercase characters which improves the accuracy of the scoring. Then, the maximum similarity score between the user query and the text blocks is chosen. This score ranges, in theory, from 0 to 100; however, in practice, most scores do not exceed ~75. Thus, the semantic scorer already introduces non-linearity.

These scores are converted to a star rating from 1.00 to 5.00, where scores less than or equal to 20 are automatically assigned a star rating of 1.00, scores from 20 to 50 scale linearly with a coefficient of 1/15 from 20 to 50 (resulting in star ratings from 1.00 to 3.00), and scores from 50 to 75 scale linearly with a coefficient of 0.08 from 50 to 75 (resulting in star ratings from 3.00 to 5.00). Scores above 75 are automatically given 5.00 star ratings. These linear coefficients arise from solving the linear equations using the chosen bounds, and these bounds are manually selected based off of empirical results from unit testing.

An issue with this updated content relevance module is that the user prompt is compared against many possible text blocks. Some webpages have hundreds of textblocks that are scraped, and this causes the URL validator to perform slowly. On such webpages, the validator can take 1-2 minutes to complete the process. This cannot be optimized in terms of speed, as the semantic scorer is an outside API call over which we have no contol. However, it would be worthwhile to apply stronger pre-processing to the text blocks to reduce the number of text blocks that are checked.

### Title Relevance

The title relevance module compares the webpage URL to the user query. The webpage URL is parsed to remove all special characters and converted to a set of unique tokens. The unique words in the user query are extracted and compared against the tokens in the URL. The star rating is then calculated as the proportion of unique words in the user query that appear in the webpage URL, normalized to produce a star rating between 1.00 and 5.00.

The title relevance module is fairly simple and is not the most reliable; however, it provides an additional metric that can be scored without access to the webpage content.

### Citation Scoring

The citation scoring module is similar to the one in deliverable 1, utilizing the same code as provided to us in the template. The module works by executing a search on Google Scholar using SerpAPI, counting the number of results which contain the webpage URL. In limited testing, the citation scoring has always been zero. It is not certain if this is because the citation scorer is nonfunctional or if it is simply because almost zero of the webpages (compared to the number of webpages on the internet) that exist on the internet are referenced in academic material.

The citation scoring is enabled by the user adding a flag when calling _rate_url_validity()_ and is automatically disabled if no SerpAPI key is provided when instantiating the URLValidator() class.

### Weighting and Final Scoring

The final star rating is a weighted average of the other scoring metrics. Since not all scores are available for all links, whether due to the citation flag being disabled or due to a failure to scrape the webpage, the weights are dynamically assigned based on the scores that are present. In general, domain trust is weighted slightly more than content relevance which are both weighted higher than citation score, and title relevance is given an extremely small weight. These weights may not be accurate and are prime subjects for further tuning to improve the performance of the URLValidator() class.

## Unit Tests

The materials used for unit testing are available under the [testing/](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/tree/main/src/deliverable-02/testing) directory. 20 prompts were generated across 5 different knowledge domains (Finance, Health, Medicine, Sports, and Travel) and can be found in [test_queries.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/testing/test_queries.csv). Each of these 20 prompts was entered into a SerpAPI search to generate 40 relevant URLs to validate. The results of these 800 validations can be found in [unit tests.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/testing/unit_tests.csv). One URL for each prompt was randomly selected to be manually scored, in order to provide a sample set comparing the scores generated by the URLValidator() class to a human "ground-truth" label, available at [sample.csv](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/testing/sample.csv).

The prompts were generated using ChatGPT, and the script used to generate the unit tests can be found at [automatic_unit_testing.py](https://github.com/JoshuaGottlieb/Chatbot-Credibility-Scorer/blob/main/src/deliverable-02/automatic_unit_testing.py).

## Future Enhancements and Auxiliary Scripts

<ul>
  <li>The creation of a script to generate the domain trust functions, as outlined in docs/Credibility_Algorithm_v3.pdf.</li>
  <li>Improvements to the scraping process of text content on the webpage. The current process casts a fairly wide net, as not all webpages are set up in an identical manner, and the previous process in deliverable 1 of only scraping paragraph tags frequently missed important content on the webpage. However, this wide net results in many text blocks being captured which causes the content relevance module to be very slow. A stricter filter to reduce the number of text blocks captured would greatly enhance the speed of the URLValidator but care is needed not to have to strict of a filter or else the quality of the predictions made will decline.</li>
  <li>Fine tuning of the weights used in the domain trust and content relevance modules, as well as the weights used in the final weighted averaging method to produce a final star rating.</li>
  <li>Improvements to the citation score module. The citation score module returns 0 for all tested links, which may mean that the citation scorer is nonfunctional. Another improvment would be to implement code which dynamically recgonizes when the citation score flag should be activated based on the user prompt, as opposed to having to manually pass in a flag.</li>
  <li>An interesting phenomenon occurred when manually rating webpages. Depending on the knowledge domain, different aspects of the webpage were more important. For Finance or Medical questions, domain authority and strength of references (outgoing links) were highly important. For Travel questions, the domain authority and outgoing links were almost irrelevant and the page content was all that mattered. It would be useful to be able to dynamically weight the different scoring modules based on the source domain, or perhaps even allow the user to specify their own weights to each scoring category.</li>
  <li>Another interesting problem arose for questions in the Health/Medical domains. Some query results returned academic papers which are highly technical and provides deep, complex answers, while other queries returned informal articles which provide surface level overviews of the topic without getting mired in detail. Depending on the user, either possibility could be good or bad. For a layperson who just wants a brief overview, an academic paper is likely overkill and not what they are looking for, but for an academic reseacher who wants strong source material, the top-level article is not rigorous enough. While this could, in theory, be covered by the Citation Scoring module, it is uncertain if that module is even functional and may end up leading to many false negatives. A potential addition to the URLValidator would be a "Technical Score" which rates how complex and technical the webpage content is. The proper way to implement such a module is not obvious, but if there was enough time to further develop this project, it could be a beneficial module to add.</li>
  <li>The creation of a Results class to hold all of the information used during each URL validation. This class would allow the program to remember the scraped data and similarity scores generated and carry it across validation sessions without having to rescrape and recalculate all of the same steps, which may be useful for further finetuning of scores by testing the same Results data using different weighting or preprocessing schema. A possible use-case for the user would be to allow the user to request different weights for the various scoring modules after seeing the results from one scoring session that could be applied swiftly without having to recalculate all of the data for each URL by scratch.</li>
  <li>A method for generating trusted domains that is automatic and pulls from an established API, as opposed to ChatGPT. Although ChatGPT's results appear accurate after inspection, it is possible for the LLM to hallucinate answers and scores for trusted domains that are not true. Unfortunately, most APIs related to domain trust are used for search engine optimization and are paid, with many of them being quite expensive for any realistic workload.</li>
  <li>The creation of a way to determine the knowledge domain relevant to the user prompt strictly from examining the user prompt (ex: Finance, Sports, Medicine, etc.). The current class requires the user to pass in the desired knowledge domain as a flag, and if no knowledge domain is passed, all knowledge domains are used. This is a topic modeling problem which may require a specialized model to be trained for this purpose.</li>
</ul>
